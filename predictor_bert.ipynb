{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c_3W14jbUIJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRqLQ3pGxSpe"
      },
      "source": [
        "# **資料蒐集**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzlQG5z6xYSG"
      },
      "outputs": [],
      "source": [
        "issue_list = [\n",
        "                '妨害風化',\n",
        "                '恐嚇危害安全',\n",
        "                '公然侮辱',\n",
        "                '誹謗',\n",
        "                '妨害秘密',\n",
        "                '妨害電腦使用'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRarAF8NxaDs"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import random\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "HEADERS = {\n",
        "  'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.92 Safari/537.36',\n",
        "}\n",
        "\n",
        "def get_case(issue, page1, page2):\n",
        "  link_list = []\n",
        "  for page in range(page1, page2):\n",
        "    url = f\"https://www.lawplus.com.tw/rest/search/report?querySentence=網路&keyword=網路&prevKeyword=網路&date=&money=&sentence=&caseNum=&caseTypes=&courts=&levels=&jtypes=&tags=&issue={issue}&main=&judge=&judgeTypes=&lawyer=&litigant=&prosecutor=&clerk=&rows=10&page={page+1}&sortField=&_=1631628688160\"\n",
        "        \n",
        "    r = requests.get(url, headers=HEADERS)\n",
        "    base_data = r.json()\n",
        "    for each in base_data['rows']:\n",
        "      link_list.append('https://www.lawplus.com.tw/rest/search/report/' + each['identifier'])\n",
        "      \n",
        "    time.sleep(1)\n",
        "\n",
        "  case_list = []\n",
        "  for link in link_list:\n",
        "    case_r = requests.get(link, headers=HEADERS)\n",
        "    case_data = case_r.json()\n",
        "    case_list.append({\n",
        "      'caseNum': case_data['response']['reportBase']['caseNum'],  #案號\n",
        "      'court': case_data['response']['reportBase']['courtCode'],  #法院\n",
        "      'issue': issue,\n",
        "      'content': case_data['response']['reportBase']['content'],\n",
        "      'defendant': case_data['response']['report']['defendant'],  #被告\n",
        "      'main': case_data['response']['report']['main'],  #主文\n",
        "      'level': case_data['response']['report']['level'],  #審級\n",
        "      'previousCaseNum': case_data['response']['report']['previousCaseNum']  #前審案號\n",
        "    })\n",
        "\n",
        "    time.sleep(1)\n",
        "  \n",
        "  return case_list\n",
        "\n",
        "\n",
        "def crawler():\n",
        "  judgement = []\n",
        "\n",
        "  for issue in issue_list:\n",
        "    psize_url = f\"https://www.lawplus.com.tw/rest/search/report?querySentence=網路&keyword=網路&prevKeyword=網路&date=&money=&sentence=&caseNum=&caseTypes=&courts=&levels=&jtypes=&tags=&issue={issue}&main=&judge=&judgeTypes=&lawyer=&litigant=&prosecutor=&clerk=&rows=10&page=1&sortField=&_=1631628688160\"  #加上全文檢索「網路」以縮小搜尋範圍\n",
        "    psize_r = requests.get(psize_url, headers=HEADERS)\n",
        "    psize_data = psize_r.json()\n",
        "    psize = psize_data['total']\n",
        "    \n",
        "    for row in range((psize//100)+1):\n",
        "      if row > (psize//100):\n",
        "        case_list = get_case(issue, (row*100), psize)\n",
        "      else:\n",
        "        case_list = get_case(issue, (row*100), (row*100+100))\n",
        "  \n",
        "      judgement.extend(case_list)\n",
        "      time.sleep(1)\n",
        "\n",
        "  df = pd.DataFrame(judgement)\n",
        "  df.to_csv('/content/drive/MyDrive/legal_predictor/data/judgement.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1awZl7Cxb5J"
      },
      "outputs": [],
      "source": [
        "#get_case('妨害風化', 0, 2)\n",
        "\n",
        "crawler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gnAdkCfxfRQ"
      },
      "source": [
        "# **資料前處理**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvVTheMgxie0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "raw = pd.read_csv('/content/drive/MyDrive/legal_predictor/data/judgement.csv', index_col=0)\n",
        "\n",
        "raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bJuHNXoxj5k"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def rm_space(data):\n",
        "  a = re.compile(r'\\n|&nbsp|\\xa0|\\\\xa0|\\u3000|\\\\u3000|\\u0020|\\\\u0020|\\t|\\r')\n",
        "  data = a.sub('', data)\n",
        "  return data\n",
        "\n",
        "\n",
        "def clean_cont(data):  # 裁判書全文處理 (content)\n",
        "  cont = []\n",
        "\n",
        "  for content in data['content']:\n",
        "    fact = re.search(r'\\n\\s+[\\u2e80-\\u9fff]{0,2}(\\u4e8b\\s*\\u5be6|\\u7406\\s*\\u7531)[\\s|\\S]*\\u4e2d\\s+\\u83ef\\s+\\u6c11\\s+\\u570b', content)\n",
        "    try:\n",
        "      rms_cont = rm_space(fact.group())\n",
        "    except:\n",
        "      rms_cont = rm_space(content)\n",
        "\n",
        "    try: \n",
        "      rms_cont1 = rms_cont.split('一、')[1]\n",
        "      rms_cont2 = rms_cont1.split('二、')[0]\n",
        "    except:\n",
        "      rms_cont2 = rms_cont\n",
        "\n",
        "    if ('引用' in rms_cont2 and '附件' in rms_cont2) or '如附件' in rms_cont2:\n",
        "      try: \n",
        "        cont1 = content.split('附件：')[1]\n",
        "        cont2 = cont1.split('一、')[1]\n",
        "        cont3 = cont2.split('二、')[0]\n",
        "      except:\n",
        "        pass\n",
        "    elif '檢察官聲請' in rms_cont2:\n",
        "      cont3 = ''\n",
        "    else:\n",
        "      cont3 = rms_cont2\n",
        "    \n",
        "    cont.append(cont3)\n",
        "\n",
        "  data['content_clean'] = cont\n",
        "  \n",
        "\n",
        "  return data\n",
        "\n",
        "#clean_cont(raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6rFJaIYxlXT"
      },
      "outputs": [],
      "source": [
        "def clean_all(raw):\n",
        "  raw1 = raw.drop(raw[raw['main'].isin(['上訴駁回。'])].index)  # 刪除上訴駁回案件\n",
        "  raw2 = raw1.dropna(subset=['main']) # 刪除主文為空值的資料\n",
        "  raw3 = raw2[~raw2['main'].str.contains('不受理。')]  # 刪除不受理案件\n",
        "  raw3.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  main_clean = []  # 處理主文空格\n",
        "  lawsuit = []  # 標記被告是否有罪，有罪為1，無罪為0\n",
        "  for main in raw3['main']:\n",
        "    main_c = rm_space(main)\n",
        "    main_clean.append(main_c)\n",
        "    if '無罪' in main:\n",
        "      lawsuit.append(0)\n",
        "    else:\n",
        "      lawsuit.append(1)\n",
        "\n",
        "  raw3['main_clean'] = main_clean\n",
        "  raw3['lawsuit'] = lawsuit\n",
        "\n",
        "  clean = clean_cont(raw3)\n",
        "\n",
        "  return clean\n",
        "\n",
        "\n",
        "#clean_all(raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mo45lp3Txmph"
      },
      "outputs": [],
      "source": [
        "def save_process(raw):\n",
        "  raw = clean_all(raw)\n",
        "  df = pd.DataFrame({\n",
        "      'text': raw['content_clean'],\n",
        "      'label': raw['lawsuit']\n",
        "  })\n",
        "      \n",
        "  df.drop(df.loc[df['text']==''].index, inplace=True)\n",
        "  df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  row=0\n",
        "  for data in df['text']:\n",
        "    if len(data) < 25:\n",
        "      df = df.drop(index=[row])\n",
        "    row += 1\n",
        "\n",
        "  df1 = df.drop_duplicates(subset=['text'])\n",
        "  df1.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  df1.to_csv('/content/drive/MyDrive/legal_predictor/data/process.csv', encoding='utf-8-sig')\n",
        "  #return len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-TcBDq5l-7P",
        "outputId": "cd3dffbe-063c-43dd-cc41-67155a06bece"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ],
      "source": [
        "save_process(raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXGHnc1FxrfJ"
      },
      "source": [
        "# **模型**\n",
        "\n",
        "https://reurl.cc/yeLp2O"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2iGe8U4xvN_"
      },
      "outputs": [],
      "source": [
        "%pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kFVpUsVx5zK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import joblib\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.model_selection import cross_val_score\n",
        "# from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ghb8Hymuas9x",
        "outputId": "62a6cd13-905a-4452-8102-0ca45b8b647a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-A0KVzHjng55QlBFgzeqC-1ArBE4RnJm\n",
            "To: /content/process.csv\n",
            "\r  0% 0.00/4.85M [00:00<?, ?B/s]\r100% 4.85M/4.85M [00:00<00:00, 75.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('process.csv', index_col=0)\n",
        "# df = data[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjXyFpBlx95f"
      },
      "outputs": [],
      "source": [
        "### DistilBERT\n",
        "# model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "\n",
        "### BERT\n",
        "# model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
        "\n",
        "# import transformers as ppb  # pytorch transformers\n",
        "# Load pretrained model/tokenizer\n",
        "# tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "# model = model_class.from_pretrained(pretrained_weights)\n",
        "\n",
        "###################################\n",
        "###################################\n",
        "\n",
        "# ckip繁中\n",
        "from transformers import (\n",
        "   BertTokenizerFast,\n",
        "   AutoModelForMaskedLM,\n",
        "   AutoModelForCausalLM,\n",
        "   AutoModelForTokenClassification,\n",
        ")\n",
        "\n",
        "# masked language model (ALBERT, BERT)\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
        "model = AutoModelForMaskedLM.from_pretrained('ckiplab/albert-tiny-chinese')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvERlDXSANi8",
        "outputId": "c8d00e85-71d6-4c0e-b577-7d8be3a4387b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        }
      ],
      "source": [
        "tokenized = df['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9Xi-lIFyKnX"
      },
      "source": [
        "### Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nz9MuA-LLX5Q",
        "outputId": "5a2b8aec-ef9a-42ca-937a-2a57a2e67ad3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1000, 50)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_len = 0\n",
        "for i in tokenized:\n",
        "   if len(i) > max_len:\n",
        "        max_len = len(i)\n",
        "\n",
        "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized])\n",
        "np.array(padded).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch8moavvyQKX"
      },
      "source": [
        "### Masking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xiv9Gc8WLgrh",
        "outputId": "36ca56a5-e242-47dc-e97d-9cb73cf07817"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1000, 50)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "attention_mask = np.where(padded != 0, 1, 0)\n",
        "attention_mask.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTyzaiw0yVQn"
      },
      "source": [
        "### Model : And Now, Deep Learning!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdlf1pRGL3-0"
      },
      "outputs": [],
      "source": [
        "input_ids = torch.tensor(padded)\n",
        "attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "with torch.no_grad():\n",
        "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHTZIKTaL-mN"
      },
      "outputs": [],
      "source": [
        "# Slice the output for the first position for all the sequences, take all hidden unit outputs\n",
        "features = last_hidden_states[0][:,0,:].numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wC9TZFF5MAZX"
      },
      "outputs": [],
      "source": [
        "labels = df['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClNXBc76Tup2"
      },
      "outputs": [],
      "source": [
        "np.save('/content/drive/My Drive/legal_predictor/model/feature_test', features)\n",
        "labels.to_csv('/content/drive/My Drive/legal_predictor/model/labels_test', encoding=\"utf-8-sig\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2N4mJM4QZo-z"
      },
      "source": [
        "# **打包**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfr-J6nyZAn7",
        "outputId": "e4a22fe7-acd7-43c0-802e-351cdd93cfcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.12.2-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 4.2 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.17\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.6 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 67.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 68.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 21.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.19 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.2\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "at5d9LQAbmVN"
      },
      "outputs": [],
      "source": [
        "def load_file():\n",
        "  import numpy as np\n",
        "  import pandas as pd\n",
        "\n",
        "  features = np.load('feature_test.npy')\n",
        "  labels = pd.read_csv('labels_test', index_col=0)\n",
        "\n",
        "  return features, labels\n",
        "\n",
        "\n",
        "def lawsuit_predict(input_text):\n",
        "  input = [input_text]\n",
        "\n",
        "  import numpy as np\n",
        "  import pandas as pd\n",
        "  import torch\n",
        "  from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "  from transformers import (\n",
        "   BertTokenizerFast,\n",
        "   AutoModelForMaskedLM,\n",
        "   AutoModelForCausalLM,\n",
        "   AutoModelForTokenClassification,\n",
        "  )\n",
        "\n",
        "  tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
        "  model = AutoModelForMaskedLM.from_pretrained('ckiplab/albert-tiny-chinese')\n",
        "\n",
        "  features, labels = load_file()\n",
        "\n",
        "  test_df = pd.DataFrame({ 'text': input })\n",
        "  tokenized = test_df['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=50)))\n",
        "\n",
        "  max_len = 0\n",
        "  for i in tokenized:\n",
        "    if len(i) > max_len:\n",
        "          max_len = len(i)\n",
        "\n",
        "  padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized])\n",
        "\n",
        "  attention_mask = np.where(padded != 0, 1, 0)\n",
        "  attention_mask.shape\n",
        "\n",
        "  input_ids = torch.tensor(padded)\n",
        "  attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "  input_feature = last_hidden_states[0][:,0,:].numpy()\n",
        "\n",
        "  lr_clf = LogisticRegression()\n",
        "  lr_clf.fit(features, labels)\n",
        "\n",
        "  result = lr_clf.predict_proba(input_feature)\n",
        "\n",
        "  return result[0][0], result[0][1]  #無罪率,有罪率 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvJvgr6qYhFk",
        "outputId": "36ab6725-5b60-4840-faff-5c37c2bf34fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-35qrkycCUtk0nXy1cP4GpqKL7vy_FvQ\n",
            "To: /content/feature_test.npy\n",
            "100% 84.5M/84.5M [00:00<00:00, 182MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-7LPSBU38ISrptSG9JkvRqnTvnaJn_g4\n",
            "To: /content/labels_test\n",
            "100% 5.90k/5.90k [00:00<00:00, 9.44MB/s]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.5068488628460055, 0.49315113715399445)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lawsuit_predict('民國110年11月1日，被告在臉書平台公開社團「小可愛」，公然貼文「你是不是有問題」等毀損他人名譽文字並標記甲，使甲感到不適')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "VRqLQ3pGxSpe",
        "5gnAdkCfxfRQ",
        "CyPlwR5EaZRc",
        "9kdJ3OjUMrde",
        "wK2aSxNayi4b",
        "0wU7WZ2ONycB",
        "tPAtgVSdKXK9",
        "-iZ5PLqVJ60Y"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
