{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-6pidCgPw4F6",
        "nCUGiyB40YXS",
        "kHev9sn3xHnu",
        "ONr6jw3eLJ85",
        "cWVkAm_353Nl",
        "kSjDejJDT0dd",
        "hJCq_Y1G57lV"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srP0thaUT5CD"
      },
      "source": [
        "【待精進】\n",
        "   \n",
        "*   如何下搜尋關鍵字\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6pidCgPw4F6"
      },
      "source": [
        "#**套件部署**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiBrW2rtGUEc"
      },
      "source": [
        "import re\n",
        "import time\n",
        "import random\n",
        "import requests\n",
        "from datetime import datetime\n",
        "\n",
        "from lxml import etree\n",
        "from lxml.html import fromstring\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfmL4R7XIPam",
        "outputId": "bbe43907-170f-4634-bd5c-da88a8050bca"
      },
      "source": [
        "%pip install ckiptagger\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "from ckiptagger import WS, POS, NER\n",
        "\n",
        "from ckiptagger import data_utils\n",
        "data_utils.download_data_gdown(\"./\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ckiptagger in /usr/local/lib/python3.7/dist-packages (0.2.1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1efHsY16pxK0lBD2gYCgCTnv1Swstq771\n",
            "To: /content/data.zip\n",
            "1.88GB [00:14, 133MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCUGiyB40YXS"
      },
      "source": [
        "# **篩選資料集**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDVjcgOhRh-q"
      },
      "source": [
        "laws = pd.DataFrame({\n",
        "    '妨害風化罪': ['罰金 網路 妨害風化罪', '拘役 網路 妨害風化罪'],\n",
        "    '恐嚇危害安全罪':['罰金 網路 恐嚇危害安全罪', '拘役 網路 恐嚇危害安全罪'],\n",
        "    '公然侮辱罪': ['罰金 網路 公然侮辱罪', '拘役 網路 公然侮辱罪'],\n",
        "    '誹謗罪':['罰金 網路 誹謗罪', '拘役 網路 誹謗罪'],\n",
        "    '妨害秘密罪':['罰金 網路 妨害秘密罪', '拘役 網路 妨害秘密罪'],\n",
        "    '妨害電腦使用罪':['罰金 網路 妨害電腦使用罪', '拘役 網路 妨害電腦使用罪']\n",
        "})\n",
        "\n",
        "#laws.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHev9sn3xHnu"
      },
      "source": [
        "#**函式**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONr6jw3eLJ85"
      },
      "source": [
        "###**爬蟲**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyRVWkqAK7iU"
      },
      "source": [
        "HEADERS = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.92 Safari/537.36',\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MAO619YK-yo"
      },
      "source": [
        "def get_news_list(keyword, page_num):\n",
        "    base_url = \"https://udn.com/api/more\"\n",
        "\n",
        "    news_list = []\n",
        "    for page in range(page_num):\n",
        "        channelId = 2\n",
        "        type_ = 'searchword'\n",
        "        query = f\"page={page+1}&id=search:{keyword}&channelId={channelId}&type={type_}&last_page={page+1}\"\n",
        "        news_list_url = base_url + '?' + query\n",
        "        # print(news_list_url)\n",
        "        # https://udn.com/api/more?page=2&id=search:網路%20公然侮辱&channelId=2&type=searchword&last_page=6\n",
        "        \n",
        "        r = requests.get(news_list_url, headers=HEADERS)\n",
        "        news_data = r.json()\n",
        "\n",
        "        for items in news_data['lists']:  #判斷社會新聞\n",
        "          if 'news/story/7321' in items['titleLink']:\n",
        "            news_list.append(items)\n",
        "          else:\n",
        "            pass\n",
        "\n",
        "        time.sleep(random.uniform(1, 2))\n",
        "\n",
        "    return news_list\n",
        "\n",
        "#get_news_list('罰金 網路 公然侮辱罪', 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNKFzehbLBdH"
      },
      "source": [
        "#def get_news_content(news_list):\n",
        "#  content = []\n",
        "#  for items in news_list:\n",
        "#    if 'https://udn.com/news/story/' in items['titleLink']:\n",
        "#      cont_r = requests.get(items['titleLink'], headers=HEADERS)\n",
        "#      cont_data = etree.HTML(cont_r.content.decode('utf-8'))\n",
        "#      cont = [etree.tostring(tar, method='text', encoding='unicode') for tar in cont_data.xpath(\"/html/body/main/div/section[2]/section/article/div/section[1]/p\")]\n",
        "       \n",
        "#      cont_str = \"\"\n",
        "#      for i in range(len(cont)):\n",
        "#        try:\n",
        "#          cont_str = cont_str + str(cont[i*2])\n",
        "#        except:\n",
        "#          pass\n",
        " \n",
        "#      content.append(cont_str)\n",
        "\n",
        "#    else:\n",
        "#      content.append(items['titleLink'])\n",
        "\n",
        "#  return content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mIP6UK_LEl5"
      },
      "source": [
        "def news_crawler(law, keyword, page_num):\n",
        "  news_list = get_news_list(keyword, page_num)\n",
        "#  content = get_news_content(news_list)\n",
        "\n",
        "  df_list = pd.DataFrame(news_list)\n",
        "  times = []\n",
        "  for items in df_list['time']:\n",
        "    times.append(str(items).split(\"'\")[3])\n",
        "  \n",
        "  df = pd.DataFrame({\n",
        "      \"law\": law,\n",
        "      \"title\": df_list['title'],\n",
        "      \"link\": df_list['titleLink'],\n",
        "      \"cont\": df_list['paragraph'],\n",
        "      \"time\": times\n",
        "  })\n",
        "\n",
        "  return df\n",
        "\n",
        "#news_crawler('公然侮辱罪', '拘役 網路 公然侮辱罪', 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWVkAm_353Nl"
      },
      "source": [
        "### **資料獲取**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DokTZ-VfLqYF"
      },
      "source": [
        "def execute(law_option, page_num):\n",
        "  exe_raw = pd.DataFrame()\n",
        "\n",
        "  for keyword in laws[law_option]:\n",
        "    try:\n",
        "      exe_raw = pd.concat([exe_raw, news_crawler(law_option, keyword, page_num)], axis=0)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  try:\n",
        "    exe = exe_raw.drop_duplicates(subset=['title'])\n",
        "    exe.index = range(len(exe))\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  return exe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSjDejJDT0dd"
      },
      "source": [
        "### **刪除重複案件**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkIqlxbxrbli"
      },
      "source": [
        "def remove_prefix(str_list):\n",
        "  reSortedList = sorted(str_list, key=len)\n",
        "  index = 1\n",
        "  result = []\n",
        "\n",
        "  for char in reSortedList:\n",
        "    result.append(char)\n",
        "    for otherChar in reSortedList[index:]:\n",
        "      match = otherChar.find(char)\n",
        "      if match == 0:\n",
        "        result.pop()\n",
        "        break\n",
        "\n",
        "    index += 1\n",
        "\n",
        "  return result\n",
        "\n",
        "#remove_prefix(['王', '王北北', '嘻嘻', '北王'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQJeng7oUEN1"
      },
      "source": [
        "def ckip(cont):\n",
        "  ws = WS(\"./data\")\n",
        "  pos = POS(\"./data\")\n",
        "  ner = NER(\"./data\")\n",
        "\n",
        "  ws_results = []\n",
        "  pos_results = []\n",
        "\n",
        "  for cont in cont:\n",
        "    ws_cont = ws([cont])\n",
        "    pos_cont = pos(ws_cont)\n",
        "    ws_results.append(ws_cont)\n",
        "    pos_results.append(pos_cont)\n",
        "\n",
        "  df_c = pd.DataFrame({\n",
        "      'ws': ws_results,\n",
        "      'pos': pos_results\n",
        "  })\n",
        "\n",
        "\n",
        "  nb_result = []\n",
        "  na_result = []\n",
        "  for i in range(len(df_c)):\n",
        "    new_word = [word for sentence in df_c.ws[i] for word in sentence]\n",
        "    new_pos = [pos for sentence in df_c.pos[i] for pos in sentence]\n",
        "\n",
        "    df_c1 = pd.DataFrame({\"word\": new_word, 'pos': new_pos})\n",
        "    \n",
        "    # nb\n",
        "    nb1 = []\n",
        "    for w in df_c1['word'][df_c1['pos']=='Nb']:\n",
        "      w = re.sub('[\\u59d3|\\u5973|\\u7537]$', '', w)\n",
        "      nb1.append(w)\n",
        "\n",
        "    nb2 = remove_prefix(nb1)\n",
        "    nb_result.append(nb2)\n",
        "\n",
        "    # na\n",
        "    na1 = []\n",
        "    for w in df_c1['word'][df_c1['pos']=='Na']:\n",
        "      w = re.sub('[\\u59d3|\\u5973|\\u7537]$', '', w)\n",
        "      na1.append(w)\n",
        "\n",
        "    na2 = remove_prefix(na1)\n",
        "    na_result.append(na2)\n",
        "\n",
        "\n",
        "  df_c_result = pd.DataFrame({\n",
        "      'nb': nb_result,\n",
        "      'na': na_result,\n",
        "      #'keyname': nb_result + na_result\n",
        "  })\n",
        "  return df_c_result\n",
        "\n",
        "\n",
        "#ckip(exe['cont'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZSmAfK7_b5_"
      },
      "source": [
        "def exe_de_dup(law_option, page_num):\n",
        "  exe = execute(law_option, page_num)\n",
        "  ckip_result = ckip(exe['cont'])\n",
        "  de_dup = exe.merge(ckip_result, how='right', left_index=True, right_index=True)\n",
        "  de_dup.sort_values('time', inplace=True)\n",
        "  de_dup = de_dup.reset_index(drop=True)\n",
        "\n",
        "  de_result = pd.DataFrame()\n",
        "  index = 1\n",
        "\n",
        "  for row in range(len(de_dup)):\n",
        "    de_result = pd.concat([de_result, de_dup[row:row+1]], axis=0)\n",
        "\n",
        "    for target in de_dup['nb'][index:]:\n",
        "      match_nb = [i for i in de_dup['nb'][row] if i in target]\n",
        "      if len(match_nb) > 1:\n",
        "        #print('nb', de_result[-1:], match_nb)\n",
        "        de_result = de_result[:-1]\n",
        "        break\n",
        "      elif len(match_nb) == 1:\n",
        "        #print('na', de_dup['nb'][row], target)\n",
        "        match_na1 = [j for j in de_dup['na'][row] if j in target]\n",
        "        match_na2 = [j for j in de_dup['nb'][row] if j in de_dup['na'][index]]\n",
        "        if (len(match_na1) > 0 and match_na1 not in match_nb) or (len(match_na2) > 0 and match_na2 not in match_nb):\n",
        "          #print('na', de_result[-1:], match_na1, match_na2)\n",
        "          de_result = de_result[:-1]\n",
        "          break\n",
        "\n",
        "\n",
        "    index += 1\n",
        "\n",
        "  de_result.drop(['nb', 'na'], axis=1)\n",
        "  de_result = de_result.reset_index(drop=True)\n",
        "  return de_result\n",
        "\n",
        "\n",
        "#exe_de_dup('公然侮辱罪', 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJCq_Y1G57lV"
      },
      "source": [
        "### **Google Sheet 輸出**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_cKzc5O6Apn"
      },
      "source": [
        "# Google Sheet 操作語法參考：https://medium.com/@fsflyingsoar/%E7%AD%86%E8%A8%98-for-python-google-colab-gspread-a397e80d071d\n",
        "\n",
        "def get_sheet(book_title, sheet_title):\n",
        "  try:\n",
        "    sh = gc.open(book_title)\n",
        "  except:\n",
        "    sh = gc.create(book_title)\n",
        "  \n",
        "  try: \n",
        "    worksheet = sh.worksheet(sheet_title)\n",
        "    worksheet.clear()\n",
        "  except:\n",
        "    worksheet = sh.add_worksheet(title=sheet_title, rows='100', cols='5')\n",
        "\n",
        "  return worksheet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2045Y8g660R"
      },
      "source": [
        "def save_sheet(exe, worksheet, count):\n",
        "  worksheet.update_cell(count+2, 1, exe['law'][count])\n",
        "  worksheet.update_cell(count+2, 2, exe['title'][count])\n",
        "  worksheet.update_cell(count+2, 3, exe['link'][count])\n",
        "  worksheet.update_cell(count+2, 4, exe['cont'][count])\n",
        "  worksheet.update_cell(count+2, 5, exe['time'][count])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib5aU3xOL77k"
      },
      "source": [
        "def sheet_set(sheet_option, exe):\n",
        "  if exe.empty:\n",
        "      print('Invalid Syntax')\n",
        "\n",
        "  else:\n",
        "    worksheet = get_sheet('news', sheet_option)\n",
        "    worksheet.update_cell(1, 1, 'law')\n",
        "    worksheet.update_cell(1, 2, 'title')\n",
        "    worksheet.update_cell(1, 3, 'link')\n",
        "    worksheet.update_cell(1, 4, 'cont')\n",
        "    worksheet.update_cell(1, 5, 'time')\n",
        "    # print(worksheet)\n",
        "\n",
        "    for count in range(len(exe)):\n",
        "      try:\n",
        "        save_sheet(exe, worksheet, count)\n",
        "      except:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lb8QT-rBcbs"
      },
      "source": [
        "def exe_to_sheet(page_num):\n",
        "  all_exe = pd.DataFrame()\n",
        "\n",
        "  for law in laws:\n",
        "    exe = exe_de_dup(law, page_num)\n",
        "    sheet_set(law, exe)\n",
        "    all_exe = pd.concat([all_exe, exe], axis=0, ignore_index=True)\n",
        "\n",
        "  sheet_set('all', all_exe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-8r-UPxyJdW"
      },
      "source": [
        "#**執行**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t33Ppqq-j5RQ"
      },
      "source": [
        "###### Google ######\n",
        "# !pip install --upgrade gspread\n",
        "\n",
        "\n",
        "###### Ckip ######\n",
        "# %pip install tensorflow\n",
        "# %pip install gdown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcJ4MGay3loG"
      },
      "source": [
        "# Google 身份驗證\n",
        "\n",
        "import gspread\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "credentials = GoogleCredentials.get_application_default()\n",
        "\n",
        "gc = gspread.authorize(credentials)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjctY7y2yPuy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b51b731-cdca-48b9-ef51-564340aa4c5d"
      },
      "source": [
        "# 新聞來源：UDN聯合新聞網\n",
        "\n",
        "exe_to_sheet(1)\n",
        "\n",
        "### 參數格式：法條, 新聞擷取頁數【新聞總筆數 = 參數*20筆*法條數】, 欲儲存資料表\n",
        "### 請注意執行程式碼的頻率"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:901: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  warnings.warn(\"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \"\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer_v1.py:1684: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}
